import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

np.random.seed(70)

# Store houses
number_of_houses = 100

# now we will set the square feet and rooms for houses
size_sqft = np.random.randint (800,3000, number_of_houses)
rooms = np.random.randint (1,6, number_of_houses)


# now we will set the prices
price = (200 * size_sqft + 15000 * rooms + 50000 + np.random.normal(0,20000, number_of_houses))


#Visualization (OPTIONAL but useful here)
 # This helps us trust the data before ML.

plt.figure()
plt.scatter(size_sqft, price)
plt.xlabel("House Size (sqft)")
plt.ylabel("Price")
plt.title("House Size vs Price")
plt.show()

# Add NOISE feature (House Color)
colors = ["red", "blue", "green", "white", "yellow"]
house_color = np.random.choice(colors, size=number_of_houses)

# Build the DataFrame (ALL together)
data_frame= pd.DataFrame({
   "size_sqft": size_sqft,
    "rooms": rooms,
    "house_color": house_color,
    "price": price

})
data_frame

# Encode categorical feature (House Color)
data_frame_encoded = pd.get_dummies(data_frame, columns=["house_color"],drop_first=True)
print(data_frame_encoded.columns)

# now we will “Go through every column name
# Keep ONLY those that start with House_color_
# Put them into a list called color_columns”
color_columns = [c for c in data_frame_encoded.columns if c.startswith("house_color_")]

# Define inputs (X) and output (y)
x = data_frame_encoded[["size_sqft", "rooms"] + color_columns]
y = data_frame_encoded["price"]

# Train / Test split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state=70)

# Train Linear Regression model
model = LinearRegression()
model.fit(x_train,y_train)

# Predict on unseen houses
y_pred = model.predict(x_test)

# Visualization (Actual vs Predicted)
plt.figure()
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("Actual vs Predicted house Price")

min_price = min(y_test.min(), y_pred.min())
max_price = max(y_test.max(), y_pred.max())
plt.plot([min_price, max_price], [min_price, max_price])
plt.show()

mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)
print("------------------------------Metrices------------------------")

print("MAE:", mae)
print("RMSE:", rmse)
print("R2:", r2)

print("----------------------\nLearned coefficients:------------------")
for name, coef in zip(x.columns, model.coef_):
    print(f"{name:25s} : {coef}")

print("\nBias (intercept):", model.intercept_)

 # we detect some noise, now we will apply ridger2 to overcome this noise
from sklearn.linear_model import Ridge

ridge_model = Ridge(alpha=1.0)
ridge_model.fit(x_train, y_train)

# Predict on test data
y_pred_ridge = ridge_model.predict(x_test)

# Evaluate Ridge model
mae_ridge = mean_absolute_error(y_test, y_pred_ridge)
rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))
r2_ridge = r2_score(y_test, y_pred_ridge)


print("------------------------------Ridge Metrices------------------------")
print("Ridge MAE:", mae_ridge)
print("Ridge RMSE:", rmse_ridge)
print("Ridge R2:", r2_ridge)


print("-------\nRidge Learned coefficients:------")
for name, coef in zip(x.columns, ridge_model.coef_):
    print(f"{name:25s} : {coef}")

print("\nRidge Bias:", ridge_model.intercept_)

# now our model has some noise because ridge can only mininimze the overfitting, now we will use lasso r1 to delete the noise



from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(x_train)
X_test_scaled = scaler.transform(x_test)

lasso_model = Lasso(alpha=1.0, max_iter=10000)
lasso_model.fit(X_train_scaled, y_train)

y_pred_lasso = lasso_model.predict(X_test_scaled)

mae_lasso = mean_absolute_error(y_test, y_pred_lasso)
rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))
r2_lasso = r2_score(y_test, y_pred_lasso)

print("------------------------------Lasso Metrices------------------------")
print("Lasso MAE:", mae_lasso)
print("Lasso RMSE:", rmse_lasso)
print("Lasso R2:", r2_lasso)


print("----------\nLasso Learned coefficients:-------")
for name, coef in zip(x.columns, lasso_model.coef_):
    print(f"{name:25s} : {coef}")

print("\nLasso Bias:", lasso_model.intercept_)

# Lasso regularization was not effective in this project because the dataset is low-dimensional and lacks strong sparsity; Ridge regularization is more appropriate for smoothly shrinking weak noisy features rather than eliminating them.
